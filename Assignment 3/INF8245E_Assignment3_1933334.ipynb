{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "voluntary-wallet",
   "metadata": {},
   "source": [
    "# INF8245E: Machine Learning | Assignment #3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intermediate-finland",
   "metadata": {},
   "source": [
    "**Louis Plessis (1933334)** | 14 November 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advised-medicine",
   "metadata": {},
   "source": [
    "# 1. Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "legal-output",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "strings = pd.read_csv(\"medical_dataset/train.csv\", header=None).to_numpy()\n",
    "strings = np.delete(strings, 0, 0)\n",
    "strings = np.delete(strings, 0, 1).flatten()\n",
    "\n",
    "import string\n",
    "trans = str.maketrans(\"\", \"\", string.punctuation)\n",
    "\n",
    "vocab = []\n",
    "\n",
    "#Removing punctuation and lower-casing words\n",
    "for i in range(len(strings)):\n",
    "    strings[i] = strings[i].translate(trans)\n",
    "    strings[i] = strings[i].lower()\n",
    "    \n",
    "    for j in strings[i].split():\n",
    "        if j not in vocab:\n",
    "            vocab.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "balanced-burton",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running this will take a long time\n",
    "\n",
    "frequency = []\n",
    "\n",
    "for i in vocab:\n",
    "    count = 0\n",
    "    for string in strings:\n",
    "        count += string.count(i)\n",
    "    frequency.append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "great-sheriff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "freqsort = np.argsort(frequency)\n",
    "\n",
    "#Writing vocabulary file\n",
    "with open('medical_text-vocab.txt', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    for i in range(10000):\n",
    "        row = str(vocab[freqsort[len(freqsort)-1-i]]) + \"\\t\" + str(i+1) + \"\\t\" + str(frequency[freqsort[len(freqsort)-1-i]])\n",
    "        writer.writerow([row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "worldwide-shore",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "vocabulary = []\n",
    "\n",
    "#Reading vocabulary file\n",
    "with open('medical_text-vocab.txt') as file:\n",
    "    reader = csv.reader(file, delimiter = \"\\t\")\n",
    "    for row in reader:\n",
    "        vocabulary.append(row)\n",
    "\n",
    "voc = np.delete(vocabulary, 2, 1)\n",
    "voc = np.delete(voc, 1, 1).flatten()\n",
    "voc = list(voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "statistical-holly",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"medical_dataset/train.csv\", header=None).to_numpy()\n",
    "test = pd.read_csv(\"medical_dataset/test.csv\", header=None).to_numpy()\n",
    "valid = pd.read_csv(\"medical_dataset/valid.csv\", header=None).to_numpy()\n",
    "\n",
    "train = np.delete(train, 0, 0)\n",
    "test = np.delete(test, 0, 0)\n",
    "valid = np.delete(valid, 0, 0)\n",
    "\n",
    "import string\n",
    "trans = str.maketrans(\"\", \"\", string.punctuation)\n",
    "\n",
    "#Removing punctuation and lower-casing words\n",
    "for i in range(len(train)):\n",
    "    train[i][1] = train[i][1].translate(trans)\n",
    "    train[i][1] = train[i][1].lower()\n",
    "    \n",
    "for i in range(len(test)):\n",
    "    test[i][1] = test[i][1].translate(trans)\n",
    "    test[i][1] = test[i][1].lower()\n",
    "    \n",
    "for i in range(len(valid)):\n",
    "    valid[i][1] = valid[i][1].translate(trans)\n",
    "    valid[i][1] = valid[i][1].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "higher-picking",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing training file\n",
    "with open('medical_text-train.txt', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    for i in range(len(train)):\n",
    "        row = \"\"\n",
    "        for word in train[i][1].split():\n",
    "            try:\n",
    "                word_index = str(voc.index(word) + 1) + \" \"\n",
    "            except:\n",
    "                word_index = \"\"\n",
    "            row += word_index\n",
    "        row = row + \"\\t\" + str(train[i][0])\n",
    "        writer.writerow([row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removed-absence",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing testing file\n",
    "with open('medical_text-test.txt', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    for i in range(len(test)):\n",
    "        row = \"\"\n",
    "        for word in test[i][1].split():\n",
    "            try:\n",
    "                word_index = str(voc.index(word) + 1) + \" \"\n",
    "            except:\n",
    "                word_index = \"\"\n",
    "            row += word_index\n",
    "        row = row + \"\\t\" + str(test[i][0])\n",
    "        writer.writerow([row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-meditation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing validation file\n",
    "with open('medical_text-valid.txt', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    for i in range(len(valid)):\n",
    "        row = \"\"\n",
    "        for word in valid[i][1].split():\n",
    "            try:\n",
    "                word_index = str(voc.index(word) + 1) + \" \"\n",
    "            except:\n",
    "                word_index = \"\"\n",
    "            row += word_index\n",
    "        row = row + \"\\t\" + str(valid[i][0])\n",
    "        writer.writerow([row])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medium-lobby",
   "metadata": {},
   "source": [
    "# 2. Binary bag-of-words (BBoW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functioning-ukraine",
   "metadata": {},
   "source": [
    "## (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surface-anxiety",
   "metadata": {},
   "source": [
    "### Random Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organic-marks",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "rapid-hayes",
   "metadata": {},
   "source": [
    "### Majority-Class Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southwest-nylon",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "registered-lending",
   "metadata": {},
   "source": [
    "## (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finished-xerox",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "choice-spyware",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "certified-sample",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soviet-importance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "buried-nebraska",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promotional-decision",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "little-regard",
   "metadata": {},
   "source": [
    "### Linear SVM/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faced-double",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fitted-senate",
   "metadata": {},
   "source": [
    "## (c) Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "answering-flooring",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "physical-clinton",
   "metadata": {},
   "source": [
    "## (d) Training, Validation, and Testing F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-basis",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "handmade-makeup",
   "metadata": {},
   "source": [
    "## (e) Performance comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungry-wichita",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "radical-burden",
   "metadata": {},
   "source": [
    "# 3. Frequency bag-of-words (FBoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sought-pocket",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
