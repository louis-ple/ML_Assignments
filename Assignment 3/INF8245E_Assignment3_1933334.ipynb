{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "incorrect-three",
   "metadata": {},
   "source": [
    "# INF8245E: Machine Learning | Assignment #3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-switzerland",
   "metadata": {},
   "source": [
    "**Louis Plessis (1933334)** | 14 November 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "provincial-hello",
   "metadata": {},
   "source": [
    "# 1. Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "joined-charger",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "strings = pd.read_csv(\"medical_dataset/train.csv\", header=None).to_numpy()\n",
    "strings = np.delete(strings, 0, 0)\n",
    "strings = np.delete(strings, 0, 1).flatten()\n",
    "\n",
    "import string\n",
    "trans = str.maketrans(\"\", \"\", string.punctuation)\n",
    "\n",
    "vocab = []\n",
    "\n",
    "#Removing punctuation and lower-casing words\n",
    "for i in range(len(strings)):\n",
    "    strings[i] = strings[i].translate(trans)\n",
    "    strings[i] = strings[i].lower()\n",
    "    \n",
    "    for j in strings[i].split():\n",
    "        if j not in vocab:\n",
    "            vocab.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "interim-grenada",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running this will take a long time\n",
    "\n",
    "frequency = []\n",
    "\n",
    "for i in vocab:\n",
    "    count = 0\n",
    "    for string in strings:\n",
    "        count += string.count(i)\n",
    "    frequency.append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "provincial-trash",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "freqsort = np.argsort(frequency)\n",
    "\n",
    "#Writing vocabulary file\n",
    "with open('medical_text-vocab.txt', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    for i in range(10000):\n",
    "        row = str(vocab[freqsort[len(freqsort)-1-i]]) + \"\\t\" + str(i+1) + \"\\t\" + str(frequency[freqsort[len(freqsort)-1-i]])\n",
    "        writer.writerow([row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "binary-panic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "vocabulary = []\n",
    "\n",
    "#Reading vocabulary file\n",
    "with open('medical_text-vocab.txt') as file:\n",
    "    reader = csv.reader(file, delimiter = \"\\t\")\n",
    "    for row in reader:\n",
    "        vocabulary.append(row)\n",
    "\n",
    "voc = np.delete(vocabulary, 2, 1)\n",
    "voc = np.delete(voc, 1, 1).flatten()\n",
    "voc = list(voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "logical-railway",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"medical_dataset/train.csv\", header=None).to_numpy()\n",
    "test = pd.read_csv(\"medical_dataset/test.csv\", header=None).to_numpy()\n",
    "valid = pd.read_csv(\"medical_dataset/valid.csv\", header=None).to_numpy()\n",
    "\n",
    "train = np.delete(train, 0, 0)\n",
    "test = np.delete(test, 0, 0)\n",
    "valid = np.delete(valid, 0, 0)\n",
    "\n",
    "import string\n",
    "trans = str.maketrans(\"\", \"\", string.punctuation)\n",
    "\n",
    "#Removing punctuation and lower-casing words\n",
    "for i in range(len(train)):\n",
    "    train[i][1] = train[i][1].translate(trans)\n",
    "    train[i][1] = train[i][1].lower()\n",
    "    \n",
    "for i in range(len(test)):\n",
    "    test[i][1] = test[i][1].translate(trans)\n",
    "    test[i][1] = test[i][1].lower()\n",
    "    \n",
    "for i in range(len(valid)):\n",
    "    valid[i][1] = valid[i][1].translate(trans)\n",
    "    valid[i][1] = valid[i][1].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "unknown-frame",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing training file\n",
    "with open('medical_text-train.txt', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    for i in range(len(train)):\n",
    "        row = \"\"\n",
    "        for word in train[i][1].split():\n",
    "            try:\n",
    "                word_index = str(voc.index(word) + 1) + \" \"\n",
    "            except:\n",
    "                word_index = \"\"\n",
    "            row += word_index\n",
    "        row = row + \"\\t\" + str(train[i][0])\n",
    "        writer.writerow([row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cordless-gabriel",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing testing file\n",
    "with open('medical_text-test.txt', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    for i in range(len(test)):\n",
    "        row = \"\"\n",
    "        for word in test[i][1].split():\n",
    "            try:\n",
    "                word_index = str(voc.index(word) + 1) + \" \"\n",
    "            except:\n",
    "                word_index = \"\"\n",
    "            row += word_index\n",
    "        row = row + \"\\t\" + str(test[i][0])\n",
    "        writer.writerow([row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "delayed-holder",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing validation file\n",
    "with open('medical_text-valid.txt', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    for i in range(len(valid)):\n",
    "        row = \"\"\n",
    "        for word in valid[i][1].split():\n",
    "            try:\n",
    "                word_index = str(voc.index(word) + 1) + \" \"\n",
    "            except:\n",
    "                word_index = \"\"\n",
    "            row += word_index\n",
    "        row = row + \"\\t\" + str(valid[i][0])\n",
    "        writer.writerow([row])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "south-engineering",
   "metadata": {},
   "source": [
    "# 2. Binary bag-of-words (BBoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "determined-obligation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "#Reading training file\n",
    "train = []\n",
    "with open('medical_text-train.txt') as file:\n",
    "    reader = csv.reader(file, delimiter = \"\\t\")\n",
    "    for row in reader:\n",
    "        train.append(row)\n",
    "        \n",
    "#Reading testing file\n",
    "test = []\n",
    "with open('medical_text-test.txt') as file:\n",
    "    reader = csv.reader(file, delimiter = \"\\t\")\n",
    "    for row in reader:\n",
    "        test.append(row)\n",
    "        \n",
    "#Reading validation file\n",
    "valid = []\n",
    "with open('medical_text-valid.txt') as file:\n",
    "    reader = csv.reader(file, delimiter = \"\\t\")\n",
    "    for row in reader:\n",
    "        valid.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "organized-tours",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frequency Bag of Words Representation\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "for i in range(len(train)):\n",
    "    a = []\n",
    "    words = train[i][0].split()\n",
    "    for j in range(10000):\n",
    "        if str(j+1) in words:\n",
    "            a.append(1)\n",
    "        else:\n",
    "            a.append(0)\n",
    "    x_train.append(a)\n",
    "    y_train.append(train[i][1])\n",
    "    \n",
    "x_test = []\n",
    "y_test = []\n",
    "for i in range(len(test)):\n",
    "    a = []\n",
    "    words = test[i][0].split()\n",
    "    for j in range(10000):\n",
    "        if str(j+1) in words:\n",
    "            a.append(1)\n",
    "        else:\n",
    "            a.append(0)\n",
    "    x_test.append(a)\n",
    "    y_test.append(test[i][1])\n",
    "    \n",
    "x_valid = []\n",
    "y_valid = []\n",
    "for i in range(len(valid)):\n",
    "    a = []\n",
    "    words = valid[i][0].split()\n",
    "    for j in range(10000):\n",
    "        if str(j+1) in words:\n",
    "            a.append(1)\n",
    "        else:\n",
    "            a.append(0)\n",
    "    x_valid.append(a)\n",
    "    y_valid.append(valid[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formal-artist",
   "metadata": {},
   "source": [
    "## (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "conservative-paint",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def computeF1ScoreA(classifier):\n",
    "    classifier = classifier.fit(x_train, y_train)\n",
    "    \n",
    "    predictions_train = classifier.predict(x_train)\n",
    "    predictions_valid = classifier.predict(x_valid)\n",
    "    predictions_test = classifier.predict(x_test)\n",
    "\n",
    "    train_score = f1_score(y_train, predictions_train, average=\"macro\")\n",
    "    valid_score = f1_score(y_valid, predictions_valid, average=\"macro\")\n",
    "    test_score = f1_score(y_test, predictions_test, average=\"macro\")\n",
    "    \n",
    "    print(\"************ F1-Scores ************\")\n",
    "    print(\"Training:\\t\", train_score)\n",
    "    print(\"Validation:\\t\", valid_score)\n",
    "    print(\"Testing:\\t\", test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reduced-report",
   "metadata": {},
   "source": [
    "### Random Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "breeding-memorial",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************ F1-Scores ************\n",
      "Training:\t 0.2477223299173802\n",
      "Validation:\t 0.23378625245294232\n",
      "Testing:\t 0.2821662562425496\n"
     ]
    }
   ],
   "source": [
    "dummy_classifier = DummyClassifier(strategy=\"uniform\")\n",
    "computeF1ScoreA(dummy_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foster-white",
   "metadata": {},
   "source": [
    "### Majority-Class Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cardiac-setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************ F1-Scores ************\n",
      "Training:\t 0.120996778472617\n",
      "Validation:\t 0.12424698795180723\n",
      "Testing:\t 0.14183381088825217\n"
     ]
    }
   ],
   "source": [
    "majority_class_classifier = DummyClassifier(strategy=\"most_frequent\")\n",
    "computeF1ScoreA(majority_class_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complimentary-escape",
   "metadata": {},
   "source": [
    "## (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "polish-extra",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def computeF1ScoreB(classifier, parameters):\n",
    "    \n",
    "    classifier = GridSearchCV(classifier, parameters, refit=True)\n",
    "    X = np.concatenate((x_train, x_valid))\n",
    "    Y = np.concatenate((y_train, y_valid))\n",
    "    classifier = classifier.fit(X, Y)\n",
    "    \n",
    "    predictions_train = classifier.predict(x_train)\n",
    "    predictions_valid = classifier.predict(x_valid)\n",
    "    predictions_test = classifier.predict(x_test)\n",
    "\n",
    "    train_score = f1_score(y_train, predictions_train, average=\"macro\")\n",
    "    valid_score = f1_score(y_valid, predictions_valid, average=\"macro\")\n",
    "    test_score = f1_score(y_test, predictions_test, average=\"macro\")\n",
    "    \n",
    "    print(\"************ F1-Scores ************\")\n",
    "    print(\"Training:\\t\", train_score)\n",
    "    print(\"Validation:\\t\", valid_score)\n",
    "    print(\"Testing:\\t\", test_score)\n",
    "    print(\"\\n*** Best hyper-parameter values ***\")\n",
    "    print(classifier.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asian-hometown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "public-aging",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************ F1-Scores ************\n",
      "Training:\t 0.5226291548213231\n",
      "Validation:\t 0.5020158838560775\n",
      "Testing:\t 0.46893502096274675\n",
      "\n",
      "*** Best hyper-parameter values ***\n",
      "{'alpha': 0.4}\n"
     ]
    }
   ],
   "source": [
    "computeF1ScoreB(BernoulliNB(), [{'alpha': np.arange(0.4,0.6,0.8)}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moral-balloon",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trained-subject",
   "metadata": {},
   "outputs": [],
   "source": [
    "computeF1ScoreB(DecisionTreeClassifier(), [{'max_depth': [i for i in range(10,30)], 'max_features': [1000*i for i in range(1,10)], 'max_leaf_nodes': [100*i for i in range(1,10)]}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sorted-placement",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wired-polymer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "circular-nowhere",
   "metadata": {},
   "source": [
    "### Linear SVM/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secondary-accused",
   "metadata": {},
   "outputs": [],
   "source": [
    "computeF1ScoreB(LinearSVC(), [{'max_iter': [100*i for i in range(11)]}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scenic-burton",
   "metadata": {},
   "source": [
    "## (c) Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "another-modem",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "resistant-cardiff",
   "metadata": {},
   "source": [
    "## (d) Training, Validation, and Testing F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chemical-reward",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "choice-space",
   "metadata": {},
   "source": [
    "## (e) Performance comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-devon",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "stable-remove",
   "metadata": {},
   "source": [
    "# 3. Frequency bag-of-words (FBoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lasting-twins",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
